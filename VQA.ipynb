{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample,shuffle\nfrom sklearn.model_selection import train_test_split\nimport requests","metadata":{"id":"0ecc69a6","execution":{"iopub.status.busy":"2022-03-31T19:41:32.509136Z","iopub.execute_input":"2022-03-31T19:41:32.509643Z","iopub.status.idle":"2022-03-31T19:41:33.618396Z","shell.execute_reply.started":"2022-03-31T19:41:32.509605Z","shell.execute_reply":"2022-03-31T19:41:33.617701Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom transformers import GPT2Tokenizer, TFGPT2Model\nfrom transformers import BertTokenizer, TFBertForQuestionAnswering\nfrom transformers import pipeline, AutoTokenizer\nfrom tensorflow.keras.layers import Multiply\nfrom tensorflow.keras.utils import to_categorical","metadata":{"id":"57caa904","execution":{"iopub.status.busy":"2022-03-31T19:41:33.619962Z","iopub.execute_input":"2022-03-31T19:41:33.620195Z","iopub.status.idle":"2022-03-31T19:41:41.922981Z","shell.execute_reply.started":"2022-03-31T19:41:33.620163Z","shell.execute_reply":"2022-03-31T19:41:41.922261Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device_name = tf.test.gpu_device_name()","metadata":{"id":"McuhMcv2r8E2","execution":{"iopub.status.busy":"2022-03-31T19:41:41.924151Z","iopub.execute_input":"2022-03-31T19:41:41.925017Z","iopub.status.idle":"2022-03-31T19:41:46.737951Z","shell.execute_reply.started":"2022-03-31T19:41:41.924983Z","shell.execute_reply":"2022-03-31T19:41:46.736535Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device_name","metadata":{"id":"Y7N6wjoxsL_Y","outputId":"1c632225-7870-41cc-b6a5-0106b43acfbd","execution":{"iopub.status.busy":"2022-03-31T19:41:46.739987Z","iopub.execute_input":"2022-03-31T19:41:46.740395Z","iopub.status.idle":"2022-03-31T19:41:46.752433Z","shell.execute_reply.started":"2022-03-31T19:41:46.740356Z","shell.execute_reply":"2022-03-31T19:41:46.751566Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Use InceptionV3 model for image feature extraction\n# Instantiate CV model feature extractor and freeze layers\nbase_model = tf.keras.applications.InceptionV3(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=(None, None, 3),\n    pooling='max',\n    classifier_activation=\"softmax\",\n)\nbase_model.trainable = False\n\n# Use BERT question answering model from Hugging Face\n# Download text feature extractor\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", )\nbertmodel = TFBertForQuestionAnswering.from_pretrained(\"bert-base-cased\")\nfeature_extraction = pipeline('feature-extraction', model=bertmodel, tokenizer=tokenizer)","metadata":{"id":"1f140c33","outputId":"86eab29b-f678-4eca-de4b-f1a629d3194e","execution":{"iopub.status.busy":"2022-03-31T19:41:46.753740Z","iopub.execute_input":"2022-03-31T19:41:46.754056Z","iopub.status.idle":"2022-03-31T19:42:11.901076Z","shell.execute_reply.started":"2022-03-31T19:41:46.754023Z","shell.execute_reply":"2022-03-31T19:42:11.900362Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.preprocessing.image as image\nfrom skimage import io\n \n# - Extract features from image\n# - Extract features from question\n# - Combine image + question features\nimgsize = (600, 600)\n\ndef extract_image(image_url):\n    imag = io.imread(image_url)\n    x = image.img_to_array(imag)\n    n = preprocess_input(x)\n    \n    # resize so all photos have same dim\n    size = imgsize\n    n = tf.keras.preprocessing.image.smart_resize(n, size)\n    return n\n\n\ndef extract_image_features(n):\n    feature_vector = base_model.predict(n) \n    return feature_vector\n\n\n# Gets the most common answer for a given sample\nfrom scipy import stats as s\ndef compute_answers(ans):\n    y = []\n    for i in ans:\n        y.append(i['answer'])\n        \n    answer = s.mode(y)[0]\n    return answer\n  \n\n# Gets most common 3000 answers out of given dataset\nfrom collections import Counter\ndef init_answer_info(data):\n    answers = []\n    for i in data:\n        for j in i['answers']:\n            answers.append(j['answer'])\n        \n    occurence_count = Counter(answers)\n    most_common = occurence_count.most_common(3000)\n    most_common_words = []\n    for i in most_common:\n        most_common_words.append(i[0])\n\n    num_answers = len(most_common_words)\n    return num_answers, most_common_words\n\n\n# function to get embeddings from questions\ndef extract_pooled_text_embeddings(data, start, stop, dictval='question', T = 45):\n    token_ids = np.array([np.zeros(T)])\n    attn_mask = np.array([np.zeros(T)])\n    seg_ids = np.array([np.zeros(T)])\n    for vq in data[start:stop]:\n        question = vq[dictval]\n        tokens = tokenizer.tokenize(question)\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n\n        padded_tokens=tokens + ['[PAD]' for _ in range(T-len(tokens))]\n        this_attn_mask=[1 if token != '[PAD]' else 0 for token in padded_tokens]\n        seg_id=[0 for _ in range(len(padded_tokens))]\n        sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\n\n        # put batch together by appending\n        try:\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n        \n        # some are too long so just cut off the ends\n        # TODO: fix code to just remove these samples from dataset\n        except: \n            print('except')\n            sent_ids = sent_ids[0:T]\n            this_attn_mask = this_attn_mask[0:T]\n            seg_id = seg_id[0:T]\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n\n            \n    # Finished compiling batch, now feed batch to model\n    token_ids = np.delete(token_ids, 0, 0)\n    attn_mask = np.delete(attn_mask, 0, 0)\n    seg_ids = np.delete(seg_ids, 0, 0)\n\n    token_ids = token_ids.astype(np.int64)\n    tattn_mask = attn_mask.astype(np.int64)\n    seg_ids = seg_ids.astype(np.int64)\n\n    hidden, pooled = bertmodel(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids, return_dict=False)\n    pooled_embeddings = np.array(pooled)\n        \n    return pooled_embeddings\n\n# function to get embeddings from answers from the data dictionary. Largely the same as above.\ndef extract_pooled_answer_embeddings(data, start, stop, dictval='answer', T = 10):\n    token_ids = np.array([np.zeros(T)])\n    attn_mask = np.array([np.zeros(T)])\n    seg_ids = np.array([np.zeros(T)])\n    for vq in data:\n        question = vq[0]\n        tokens = tokenizer.tokenize(question)\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n#         print(f\"Final Length: {len(tokens)}\")\n\n        padded_tokens=tokens + ['[PAD]' for _ in range(T-len(tokens))]\n        this_attn_mask=[1 if token != '[PAD]' else 0 for token in padded_tokens  ]\n        seg_id=[0 for _ in range(len(padded_tokens))]\n        sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\n\n        try:\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n        except: \n            print('answer except')\n            sent_ids = sent_ids[0:T]\n            this_attn_mask = this_attn_mask[0:T]\n            seg_id = seg_id[0:T]\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n\n    # Finished compiling batch, now feed batch to model\n    token_ids = np.delete(token_ids, 0, 0)\n    attn_mask = np.delete(attn_mask, 0, 0)\n    seg_ids = np.delete(seg_ids, 0, 0)\n\n    token_ids = token_ids.astype(np.int64)\n    tattn_mask = attn_mask.astype(np.int64)\n    seg_ids = seg_ids.astype(np.int64)\n\n    hidden, pooled = bertmodel(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids, return_dict=False)\n    pooled_embeddings = np.array(pooled)\n        \n    return pooled_embeddings\n\n\n# function to get embeddings from answers in the answer bank in list format. Largely the same as above.\n# TODO: rework functions lol\ndef extract_pooled_answerbank_embeddings(data, start, stop, dictval='answer', T = 10):\n    token_ids = np.array([np.zeros(T)])\n    attn_mask = np.array([np.zeros(T)])\n    seg_ids = np.array([np.zeros(T)])\n    for question in data:\n        tokens = tokenizer.tokenize(question)\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n#         print(f\"Final Length: {len(tokens)}\")\n\n        padded_tokens=tokens + ['[PAD]' for _ in range(T-len(tokens))]\n        this_attn_mask=[1 if token != '[PAD]' else 0 for token in padded_tokens  ]\n        seg_id=[0 for _ in range(len(padded_tokens))]\n        sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\n\n        try:\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n        except: \n            print('answer except')\n            sent_ids = sent_ids[0:T]\n            this_attn_mask = this_attn_mask[0:T]\n            seg_id = seg_id[0:T]\n            token_ids = np.vstack([token_ids, sent_ids])\n            attn_mask = np.vstack([attn_mask, this_attn_mask])\n            seg_ids = np.vstack([seg_ids, seg_id])\n\n    # Finished compiling batch, now feed batch to model\n    token_ids = np.delete(token_ids, 0, 0)\n    attn_mask = np.delete(attn_mask, 0, 0)\n    seg_ids = np.delete(seg_ids, 0, 0)\n\n    token_ids = token_ids.astype(np.int64)\n    tattn_mask = attn_mask.astype(np.int64)\n    seg_ids = seg_ids.astype(np.int64)\n\n    hidden, pooled = bertmodel(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids, return_dict=False)\n    pooled_embeddings = np.array(pooled)\n        \n    return pooled_embeddings","metadata":{"id":"8d422f72","execution":{"iopub.status.busy":"2022-03-31T19:42:11.902735Z","iopub.execute_input":"2022-03-31T19:42:11.903237Z","iopub.status.idle":"2022-03-31T19:42:12.303528Z","shell.execute_reply.started":"2022-03-31T19:42:11.903199Z","shell.execute_reply":"2022-03-31T19:42:12.302842Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Function to extract all features and concat into single vector for each sample\ndef get_feature_vectors(data, start = 0, stop = 50):\n    img_train = np.zeros((1, imgsize[0], imgsize[1], 3))\n    # Extract features describing the image\n    for i, vq in enumerate(data[start:stop]):\n          image_name = vq['image']\n          image_url = img_dir + image_name\n          image_vec = extract_image(image_url)\n          n1, n2, n3 = image_vec.shape\n          image_vec = np.reshape(image_vec, (1, n1, n2, n3))\n          img_train = np.vstack([img_train, image_vec])\n\n    img_train = np.delete(img_train, 0, 0)    \n    image_feature = extract_image_features(img_train)\n\n    # Extract features describing the question\n    question_feature = extract_pooled_text_embeddings(data, start, stop)\n        \n    # Create a multimodal feature to represent both the image and question\n    multimodal_features = np.concatenate([question_feature, image_feature], axis=1)\n\n    # Each sample has 10 manually entered answers. Get the mode of each sample's answers and use that. For now.\n    vq = data[start]\n    answers = vq['answers']\n    label = compute_answers(answers)\n    y=label\n    for vq in data[start+1:stop]:\n        answers = vq['answers']\n        label = compute_answers(answers)\n        y = np.vstack([y, label])\n        \n    return multimodal_features, y","metadata":{"id":"c67dadba","execution":{"iopub.status.busy":"2022-03-31T19:42:12.304818Z","iopub.execute_input":"2022-03-31T19:42:12.305208Z","iopub.status.idle":"2022-03-31T19:42:12.315046Z","shell.execute_reply.started":"2022-03-31T19:42:12.305172Z","shell.execute_reply":"2022-03-31T19:42:12.313548Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Download training data and create answer bank \nimg_dir = \"https://vizwiz.cs.colorado.edu//VizWiz_visualization_img/\"\nsplit = 'train' \nannotation_file = \"https://ivc.ischool.utexas.edu/VizWiz_final/vqa_data/Annotations/%s.json\" %split\n\nsplit_data = requests.get(annotation_file, allow_redirects=True)\ndata = split_data.json()\n\n# top_train_answers is the answer bank / list of most common 3000 answers\nnum_top_answers, top_train_answers = init_answer_info(data)  ","metadata":{"id":"650e9e07","outputId":"e67c4d32-1615-4ec9-c0cb-8b5cd31fa08f","execution":{"iopub.status.busy":"2022-03-31T19:42:12.316341Z","iopub.execute_input":"2022-03-31T19:42:12.316566Z","iopub.status.idle":"2022-03-31T19:42:13.568675Z","shell.execute_reply.started":"2022-03-31T19:42:12.316534Z","shell.execute_reply":"2022-03-31T19:42:13.567945Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# extract features from the training samples\nmax_ans_length = 10\nX_train = np.zeros([1, 2093])  # 2093 is the length of the final multimodal feature vector\ny_train = np.zeros(max_ans_length)\n\n# batch size of 50\nfor start in range(0, 15000, 50):\n    print(start)\n    stop = start+50\n    X_train_iter, y_train_iter = get_feature_vectors(data, start=start, stop=stop)\n\n    # get answer label embeddings\n    y_embed = extract_pooled_answer_embeddings(y_train_iter, start, stop, dictval='answer', T = max_ans_length)\n    \n    # append and save each iteration\n    X_train = np.vstack([X_train, X_train_iter])\n    y_train = np.vstack([y_train, y_embed])\n    np.save('trainX_data', X_train)\n    np.save('trainy_data', y_train)\n\n# delete the top row of np.zeros and save again\nX_train = np.delete(X_train, 0, 0)\ny_train = np.delete(y_train, 0, 0)\n\nnp.save('trainX_data', X_train)\nnp.save('trainy_data', y_train)","metadata":{"execution":{"iopub.execute_input":"2022-03-31T01:05:43.773640Z","iopub.status.busy":"2022-03-31T01:05:43.773327Z","iopub.status.idle":"2022-03-31T01:40:56.425687Z","shell.execute_reply":"2022-03-31T01:40:56.424767Z","shell.execute_reply.started":"2022-03-31T01:05:43.773604Z"},"id":"HC8f_rOLeH75","outputId":"d04df461-b758-4bc9-9fcc-a47bd065d1b6","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeat above cells for validation and test data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean data\n\n# Remove training samples where label is not in the answer bank or that are 'unanswerable', 'unsuitable', 'unsuitable image'\ndef remove_unanswerable_samples(X_train, y_train, embeddings_to_remove = [0, 1, 12]):\n    count = 0\n    new_X = []\n    new_y = []\n    \n    for X, y in zip(X_train, y_train):\n        exit = 0\n        for vec in embeddings_to_remove:\n            if (y == question_feature[vec]).all():\n                count = count+1\n                exit = 1\n                continue\n        if exit == 1:\n            continue\n        new_X.append(X)\n        new_y.append(y)\n            \n    print(count)\n    return np.array(new_X), np.array(new_y)\n\n# Get embeddings for answer bank\nmax_answer_length = 10\nd = top_train_answers\nquestion_feature = extract_pooled_answerbank_embeddings(d, 0, len(d), dictval='answer', T = max_answer_length)\n\n# these are the embeddings corresponding to 'unanswerable', 'unsuitable', 'unsuitable image'\nembeddings_to_remove = [0, 1, 12]\nX_train, y_train = remove_unanswerable_samples(X_train, y_train, embeddings_to_remove = [0, 1, 12])\n\nnp.save('trainX_data_latest', X_train)\nnp.save('trainy_data_latest', y_train)","metadata":{"id":"bf606f99","execution":{"iopub.status.busy":"2022-03-31T19:42:13.570887Z","iopub.execute_input":"2022-03-31T19:42:13.571170Z","iopub.status.idle":"2022-03-31T19:42:13.577416Z","shell.execute_reply.started":"2022-03-31T19:42:13.571134Z","shell.execute_reply":"2022-03-31T19:42:13.576712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train = np.load('../input/bert-answer-embeddings/trainX_data_latest.npy')\ny_train = np.load('../input/bert-answer-embeddings/trainy_data_latest.npy')\ny_labels = np.load('../input/bert-answer-embeddings/trainlabels_latest.npy')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T21:24:01.887242Z","iopub.execute_input":"2022-03-31T21:24:01.887500Z","iopub.status.idle":"2022-03-31T21:24:01.939891Z","shell.execute_reply.started":"2022-03-31T21:24:01.887470Z","shell.execute_reply":"2022-03-31T21:24:01.939146Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# Now create the multimodal model\nimport keras\nfrom keras import layers, Input, Model, optimizers\n\nmax_answer_length = 10  # 10 output layer nodes corresponding to 1x10 BERT text embedding\n\ninputs = Input(shape=(1, X_train.shape[1]))\nx = layers.Bidirectional(layers.LSTM(100))(inputs)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Flatten()(x)\nout = layers.Dense(max_answer_length)(x) \n\nmodel = Model(inputs=inputs, outputs=out)\nmodel.compile(\n  optimizer = 'adam',\n  loss=tf.keras.losses.MeanSquaredError(),\n  metrics=['accuracy'],\n)\n\n\nmodel.summary()","metadata":{"id":"16fdf193","execution":{"iopub.status.busy":"2022-03-31T21:24:03.224032Z","iopub.execute_input":"2022-03-31T21:24:03.224562Z","iopub.status.idle":"2022-03-31T21:24:03.680523Z","shell.execute_reply.started":"2022-03-31T21:24:03.224528Z","shell.execute_reply":"2022-03-31T21:24:03.679691Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# Reshape inputs\nn1, n2 = X_train.shape\nX_train =  X_train.reshape(n1, 1, n2)\ny_train =  y_train.reshape(n1, max_answer_length)\n\n# n1, n2 = X_val.shape\n# X_val = X_val.reshape(n1, 1, n2)\n# y_val_index = y_val_index.reshape(n1,  max_answer_length)","metadata":{"id":"2ebe2786","execution":{"iopub.status.busy":"2022-03-31T21:24:04.975998Z","iopub.execute_input":"2022-03-31T21:24:04.976554Z","iopub.status.idle":"2022-03-31T21:24:04.981431Z","shell.execute_reply.started":"2022-03-31T21:24:04.976516Z","shell.execute_reply":"2022-03-31T21:24:04.980294Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"%%timeit\n# Train the model\ninfo = model.fit(X_train, y_train, batch_size=1000, epochs=100, )","metadata":{"id":"5332fcca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nenc = LabelEncoder()\ny = enc.fit_transform(top_train_answers)\nX = question_feature\n\n# setup kNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X, y)\n\n# make predictions and reverse map with kNN\npredictions = model.predict(X_train)\nknn_preds = knn.predict(predictions)\nfinal_preds = enc.inverse_transform(knn_preds)\nresults = final_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-31T21:31:55.880436Z","iopub.execute_input":"2022-03-31T21:31:55.880968Z","iopub.status.idle":"2022-03-31T21:31:55.897876Z","shell.execute_reply.started":"2022-03-31T21:31:55.880931Z","shell.execute_reply":"2022-03-31T21:31:55.897166Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\na = pd.DataFrame(results)\na.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T22:02:59.797122Z","iopub.execute_input":"2022-03-31T22:02:59.797671Z","iopub.status.idle":"2022-03-31T22:02:59.809178Z","shell.execute_reply.started":"2022-03-31T22:02:59.797636Z","shell.execute_reply":"2022-03-31T22:02:59.808361Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_train, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_labels","metadata":{"execution":{"iopub.status.busy":"2022-03-31T20:50:15.673492Z","iopub.execute_input":"2022-03-31T20:50:15.674110Z","iopub.status.idle":"2022-03-31T20:50:15.680137Z","shell.execute_reply.started":"2022-03-31T20:50:15.674071Z","shell.execute_reply":"2022-03-31T20:50:15.679340Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-03-31T20:50:11.258617Z","iopub.execute_input":"2022-03-31T20:50:11.258862Z","iopub.status.idle":"2022-03-31T20:50:11.264564Z","shell.execute_reply.started":"2022-03-31T20:50:11.258834Z","shell.execute_reply":"2022-03-31T20:50:11.263926Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# All answers\ngtlist = [x['answers'] for x in data]\n\n# Save the accuracies\nacc_list = []\ni = 0\n\n\n# Compute accuracy for each image\nfor pred in results:\n\n    # Get the GT answer list and preprocess\n    gt_ans = gtlist[i] \n    gt_ans = [x['answer'] for x in gt_ans]\n    gt_ans = [x.lower() for x in gt_ans]\n\n    # Compute accuracy (compare with at least 3 human answers)\n    cur_acc = np.minimum(1.0, gt_ans.count(pred)/3.0)\n\n    acc_list.append(cur_acc)\n    i +=1\n\nprint ('Accuracy: {}'.format(round(np.mean(acc_list), 2)))\n\n## save results to results.csv\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.to_csv(\"results.csv\", header = None, index = None)","metadata":{"id":"431dc63d","outputId":"386df377-59c9-431c-848d-14672cbae5da","execution":{"iopub.status.busy":"2022-03-31T22:04:18.375740Z","iopub.execute_input":"2022-03-31T22:04:18.376001Z","iopub.status.idle":"2022-03-31T22:04:18.443293Z","shell.execute_reply.started":"2022-03-31T22:04:18.375971Z","shell.execute_reply":"2022-03-31T22:04:18.442609Z"},"trusted":true},"execution_count":189,"outputs":[]}]}